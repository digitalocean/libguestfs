.\" Automatically generated by Podwrapper::Man 1.18.10 (Pod::Simple 3.20)
.\"
.\" Standard preamble:
.\" ========================================================================
.de Sp \" Vertical space (when we can't use .PP)
.if t .sp .5v
.if n .sp
..
.de Vb \" Begin verbatim text
.ft CW
.nf
.ne \\$1
..
.de Ve \" End verbatim text
.ft R
.fi
..
.\" Set up some character translations and predefined strings.  \*(-- will
.\" give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
.\" double quote, and \*(R" will give a right double quote.  \*(C+ will
.\" give a nicer C++.  Capital omega is used to do unbreakable dashes and
.\" therefore won't be available.  \*(C` and \*(C' expand to `' in nroff,
.\" nothing in troff, for use with C<>.
.tr \(*W-
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.ie n \{\
.    ds -- \(*W-
.    ds PI pi
.    if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.    if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\"  diablo 12 pitch
.    ds L" ""
.    ds R" ""
.    ds C` ""
.    ds C' ""
'br\}
.el\{\
.    ds -- \|\(em\|
.    ds PI \(*p
.    ds L" ``
.    ds R" ''
'br\}
.\"
.\" Escape single quotes in literal strings from groff's Unicode transform.
.ie \n(.g .ds Aq \(aq
.el       .ds Aq '
.\"
.\" If the F register is turned on, we'll generate index entries on stderr for
.\" titles (.TH), headers (.SH), subsections (.SS), items (.Ip), and index
.\" entries marked with X<> in POD.  Of course, you'll have to process the
.\" output yourself in some meaningful fashion.
.ie \nF \{\
.    de IX
.    tm Index:\\$1\t\\n%\t"\\$2"
..
.    nr % 0
.    rr F
.\}
.el \{\
.    de IX
..
.\}
.\" ========================================================================
.\"
.IX Title "guestfs-performance 1"
.TH guestfs-performance 1 "2012-10-30" "libguestfs-1.18.10" "Virtualization Support"
.\" For nroff, turn off justification.  Always turn off hyphenation; it makes
.\" way too many mistakes in technical documents.
.if n .ad l
.nh
.SH "НАЗВА"
.IX Header "НАЗВА"
guestfs-performance \- engineering libguestfs for greatest performance
.SH "ОПИС"
.IX Header "ОПИС"
This page documents how to get the greatest performance out of libguestfs,
especially when you expect to use libguestfs to manipulate thousands of
virtual machines or disk images.
.PP
Three main areas are covered. Libguestfs runs an appliance (a small Linux
distribution) inside qemu/KVM.  The first two areas are: minimizing the time
taken to start this appliance, and the number of times the appliance has to
be started.  The third area is shortening the time taken for inspection of
VMs.
.SH "BASELINE MEASUREMENTS"
.IX Header "BASELINE MEASUREMENTS"
Before making changes to how you use libguestfs, take baseline measurements.
.SS "\s-1BASELINE:\s0 \s-1STARTING\s0 \s-1THE\s0 \s-1APPLIANCE\s0"
.IX Subsection "BASELINE: STARTING THE APPLIANCE"
On an unloaded machine, time how long it takes to start up the appliance:
.PP
.Vb 1
\& time guestfish \-a /dev/null run
.Ve
.PP
Run this command several times in a row and discard the first few runs, so
that you are measuring a typical \*(L"hot cache\*(R" case.
.PP
\fIПояснення\fR
.IX Subsection "Пояснення"
.PP
This command starts up the libguestfs appliance on a null disk, and then
immediately shuts it down.  The first time you run the command, it will
create an appliance and cache it (usually under \f(CW\*(C`/var/tmp/.guestfs\-*\*(C'\fR).
Subsequent runs should reuse the cached appliance.
.PP
\fIExpected results\fR
.IX Subsection "Expected results"
.PP
You should expect to be getting times under 6 seconds.  If the times you see
on an unloaded machine are above this, then see the section
\&\*(L"\s-1TROUBLESHOOTING\s0 \s-1POOR\s0 \s-1PERFORMANCE\s0\*(R" below.
.SS "\s-1BASELINE:\s0 \s-1PERFORMING\s0 \s-1INSPECTION\s0 \s-1OF\s0 A \s-1GUEST\s0"
.IX Subsection "BASELINE: PERFORMING INSPECTION OF A GUEST"
For this test you will need an unloaded machine and at least one real guest
or disk image.  If you are planning to use libguestfs against only X guests
(eg. X = Windows), then using an X guest here would be most appropriate.  If
you are planning to run libguestfs against a mix of guests, then use a mix
of guests for testing here.
.PP
Time how long it takes to perform inspection and mount the disks of the
guest.  Use the first command if you will be using disk images, and the
second command if you will be using libvirt.
.PP
.Vb 1
\& time guestfish \-\-ro \-a disk.img \-i exit
\&
\& time guestfish \-\-ro \-d GuestName \-i exit
.Ve
.PP
Run the command several times in a row and discard the first few runs, so
that you are measuring a typical \*(L"hot cache\*(R" case.
.PP
\fIПояснення\fR
.IX Subsection "Пояснення"
.PP
This command starts up the libguestfs appliance on the named disk image or
libvirt guest, performs libguestfs inspection on it (see
\&\*(L"\s-1INSPECTION\s0\*(R" in \fIguestfs\fR\|(3)), mounts the guest's disks, then discards all these
results and shuts down.
.PP
The first time you run the command, it will create an appliance and cache it
(usually under \f(CW\*(C`/var/tmp/.guestfs\-*\*(C'\fR).  Subsequent runs should reuse the
cached appliance.
.PP
\fIExpected results\fR
.IX Subsection "Expected results"
.PP
You should expect times which are ≤ 5 seconds greater than measured in
the first baseline test above.  (For example, if the first baseline test ran
in 5 seconds, then this test should run in ≤ 10 seconds).
.SH "UNDERSTANDING THE APPLIANCE AND WHEN IT IS BUILT/CACHED"
.IX Header "UNDERSTANDING THE APPLIANCE AND WHEN IT IS BUILT/CACHED"
The first time you use libguestfs, it will build and cache an appliance.
This is usually in \f(CW\*(C`/var/tmp/.guestfs\-*\*(C'\fR, unless you have set \f(CW$TMPDIR\fR in
which case it will be under that temporary directory.
.PP
For more information about how the appliance is constructed, see
\&\*(L"\s-1SUPERMIN\s0 \s-1APPLIANCES\s0\*(R" in \fIfebootstrap\fR\|(8).
.PP
Every time libguestfs runs it will check that no host files used by the
appliance have changed.  If any have, then the appliance is rebuilt.  This
usually happens when a package is installed or updated on the host
(eg. using programs like \f(CW\*(C`yum\*(C'\fR or \f(CW\*(C`apt\-get\*(C'\fR).  The reason for
reconstructing the appliance is security: the new program that has been
installed might contain a security fix, and so we want to include the fixed
program in the appliance automatically.
.PP
These are the performance implications:
.IP "\(bu" 4
The process of building (or rebuilding) the cached appliance is slow, and
you can avoid this happening by using a fixed appliance (see below).
.IP "\(bu" 4
If not using a fixed appliance, be aware that updating software on the host
will cause a one time rebuild of the appliance.
.IP "\(bu" 4
\&\f(CW\*(C`/var/tmp\*(C'\fR (or \f(CW$TMPDIR\fR) should be on a fast disk, and have plenty of
space for the appliance.
.SH "USING A FIXED APPLIANCE"
.IX Header "USING A FIXED APPLIANCE"
To fully control when the appliance is built, you can build a fixed
appliance.  This appliance can and should be stored on a fast, local disk.
.PP
To build the appliance, run the command:
.PP
.Vb 1
\& libguestfs\-make\-fixed\-appliance <directory>
.Ve
.PP
replacing \f(CW\*(C`<directory>\*(C'\fR with the name of a directory where the
appliance will be stored (normally you would name a subdirectory, for
example: \f(CW\*(C`/usr/local/lib/guestfs/appliance\*(C'\fR or \f(CW\*(C`/dev/shm/appliance\*(C'\fR).
.PP
Then set \f(CW$LIBGUESTFS_PATH\fR (and ensure this environment variable is set in
your libguestfs program), or modify your program so it calls
\&\f(CW\*(C`guestfs_set_path\*(C'\fR.  For example:
.PP
.Vb 1
\& export LIBGUESTFS_PATH=/usr/local/lib/guestfs/appliance
.Ve
.PP
Now you can run libguestfs programs, virt tools, guestfish etc. as normal.
The programs will use your fixed appliance, and will not ever build,
rebuild, or cache their own appliance.
.PP
(For detailed information on this subject, see:
\&\fIlibguestfs\-make\-fixed\-appliance\fR\|(1)).
.SS "\s-1PERFORMANCE\s0 \s-1OF\s0 \s-1THE\s0 \s-1FIXED\s0 \s-1APPLIANCE\s0"
.IX Subsection "PERFORMANCE OF THE FIXED APPLIANCE"
In our testing we did not find that using a fixed appliance gave any
measurable performance benefit, even when the appliance was located in
memory (ie. on \f(CW\*(C`/dev/shm\*(C'\fR).  However there are three points to consider:
.IP "1." 4
Using a fixed appliance stops libguestfs from ever rebuilding the appliance,
meaning that libguestfs will have more predictable start-up times.
.IP "2." 4
By default libguestfs (or rather, \fIfebootstrap\-supermin\-helper\fR\|(8))
searches over the root filesystem to find out if any host files have changed
and if it needs to rebuild the appliance.  If these files are not cached and
the root filesystem is on an \s-1HDD\s0, then this generates lots of seeks.  Using
a fixed appliance avoids all this.
.IP "3." 4
The appliance is loaded on demand.  A simple test such as:
.Sp
.Vb 1
\& time guestfish \-a /dev/null run
.Ve
.Sp
does not load very much of the appliance.  A real libguestfs program using
complicated \s-1API\s0 calls would demand-load a lot more of the appliance.  Being
able to store the appliance in a specified location makes the performance
more predictable.
.SH "REDUCING THE NUMBER OF TIMES THE APPLIANCE IS LAUNCHED"
.IX Header "REDUCING THE NUMBER OF TIMES THE APPLIANCE IS LAUNCHED"
By far the most effective, though not always the simplest way to get good
performance is to ensure that the appliance is launched the minimum number
of times.  This will probably involve changing your libguestfs application.
.PP
Try to call \f(CW\*(C`guestfs_launch\*(C'\fR at most once per virtual machine.
.PP
Instead of using a separate instance of \fIguestfish\fR\|(1) to make a series of
changes to the same guest, use a single instance of guestfish and/or use the
guestfish \fI\-\-listen\fR option.
.PP
Consider writing your program as a daemon which holds a guest open while
making a series of changes.  Or marshal all the operations you want to
perform before opening the guest.
.PP
You can also try adding disks from multiple guests to a single appliance.
Before trying this, note the following points:
.IP "1." 4
Adding multiple guests to one appliance is a security problem because it may
allow one guest to interfere with the disks of another guest.  Only do it if
you trust all the guests, or if you can group guests by trust.
.IP "2." 4
In current qemu, there is a limit of around 26 disks that can be added to
the appliance.  In future versions of qemu (and hence libguestfs)  we hope
to lift this limit.
.IP "3." 4
Using libguestfs this way is complicated.  Disks can have unexpected
interactions: for example, if two guests use the same \s-1UUID\s0 for a filesystem
(because they were cloned), or have volume groups with the same name (but
see \f(CW\*(C`guestfs_lvm_set_filter\*(C'\fR).
.PP
\&\fIvirt\-df\fR\|(1) adds multiple disks by default, so the source code for this
program would be a good place to start.
.SH "SHORTENING THE TIME TAKEN FOR INSPECTION OF VMs"
.IX Header "SHORTENING THE TIME TAKEN FOR INSPECTION OF VMs"
The main advice is obvious: Do not perform inspection (which is expensive)
unless you need the results.
.PP
If you previously performed inspection on the guest, then it may be safe to
cache and reuse the results from last time.
.PP
Some disks don't need to be inspected at all: for example, if you are
creating a disk image, or if the disk image is not a \s-1VM\s0, or if the disk
image has a known layout.
.PP
Even when basic inspection (\f(CW\*(C`guestfs_inspect_os\*(C'\fR) is required, auxiliary
inspection operations may be avoided:
.IP "\(bu" 4
Mounting disks is only necessary to get further filesystem information.
.IP "\(bu" 4
Listing applications (\f(CW\*(C`guestfs_inspect_list_applications\*(C'\fR) is an expensive
operation on Linux, but almost free on Windows.
.IP "\(bu" 4
Generating a guest icon (\f(CW\*(C`guestfs_inspect_get_icon\*(C'\fR) is cheap on Linux but
expensive on Windows.
.SH "TROUBLESHOOTING POOR PERFORMANCE"
.IX Header "TROUBLESHOOTING POOR PERFORMANCE"
.SS "\s-1ENSURE\s0 \s-1HARDWARE\s0 \s-1VIRTUALIZATION\s0 \s-1IS\s0 \s-1AVAILABLE\s0"
.IX Subsection "ENSURE HARDWARE VIRTUALIZATION IS AVAILABLE"
Use \f(CW\*(C`/proc/cpuinfo\*(C'\fR and this page:
.PP
http://virt\-tools.org/learning/check\-hardware\-virt/
.PP
to ensure that hardware virtualization is available.  Note that you may need
to enable it in your \s-1BIOS\s0.
.PP
Hardware virt is not usually available inside VMs, and libguestfs will run
slowly inside another virtual machine whatever you do.  Nested
virtualization does not work well in our experience, and is certainly no
substitute for running libguestfs on baremetal.
.SS "\s-1ENSURE\s0 \s-1KVM\s0 \s-1IS\s0 \s-1AVAILABLE\s0"
.IX Subsection "ENSURE KVM IS AVAILABLE"
Ensure that \s-1KVM\s0 is enabled and available to the user that will run
libguestfs.  It should be safe to set 0666 permissions on \f(CW\*(C`/dev/kvm\*(C'\fR and
most distributions now do this.
.SS "\s-1PROCESSORS\s0 \s-1TO\s0 \s-1AVOID\s0"
.IX Subsection "PROCESSORS TO AVOID"
Avoid processors that don't have hardware virtualization, and some
processors which are simply very slow (\s-1AMD\s0 Geode being a great example).
.SH "DETAILED TIMINGS USING ANNOTATE"
.IX Header "DETAILED TIMINGS USING ANNOTATE"
Use the \fIannotate\fR\|(1)/\fIannotate\-output\fR\|(1) command to show detailed
timings:
.PP
.Vb 7
\& $ annotate\-output +\*(Aq%T.%N\*(Aq guestfish \-a /dev/null run \-v
\& 22:17:53.215784625 I: Started guestfish \-a /dev/null run \-v
\& 22:17:53.240335409 E: libguestfs: [00000ms] febootstrap\-supermin\-helper \-\-verbose \-f checksum \*(Aq/usr/lib64/guestfs/supermin.d\*(Aq x86_64
\& 22:17:53.266857866 E: supermin helper [00000ms] whitelist = (not specified), host_cpu = x86_64, kernel = (null), initrd = (null), appliance = (null)
\& 22:17:53.272704072 E: supermin helper [00000ms] inputs[0] = /usr/lib64/guestfs/supermin.d
\& 22:17:53.276528651 E: checking modpath /lib/modules/3.4.0\-1.fc17.x86_64.debug is a directory
\& [etc]
.Ve
.PP
The timestamps are \f(CW\*(C`hours:minutes:seconds.nanoseconds\*(C'\fR.  By comparing the
timestamps you can see exactly how long each operation in the boot sequence
takes.
.SH "DETAILED TIMINGS USING SYSTEMTAP"
.IX Header "DETAILED TIMINGS USING SYSTEMTAP"
You can use SystemTap (\fIstap\fR\|(1)) to get detailed timings from libguestfs
programs.
.PP
Save the following script as \f(CW\*(C`time.stap\*(C'\fR:
.PP
.Vb 1
\& global last;
\& 
\& function display_time () {
\&       now = gettimeofday_us ();
\&       delta = 0;
\&       if (last > 0)
\&             delta = now \- last;
\&       last = now;
\& 
\&       printf ("%d (+%d):", now, delta);
\& }
\& 
\& probe begin {
\&       last = 0;
\&       printf ("ready\en");
\& }
\& 
\& /* Display all calls to static markers. */
\& probe process("/usr/lib*/libguestfs.so.0")
\&           .provider("guestfs").mark("*") ? {
\&       display_time();
\&       printf ("\et%s %s\en", $$name, $$parms);
\& }
\& 
\& /* Display all calls to guestfs_* functions. */
\& probe process("/usr/lib*/libguestfs.so.0")
\&           .function("guestfs_[a\-z]*") ? {
\&       display_time();
\&       printf ("\et%s %s\en", probefunc(), $$parms);
\& }
.Ve
.PP
Run it as root in one window:
.PP
.Vb 2
\& # stap time.stap
\& ready
.Ve
.PP
It prints \*(L"ready\*(R" when SystemTap has loaded the program.  Run your
libguestfs program, guestfish or a virt tool in another window.  For
example:
.PP
.Vb 1
\& $ guestfish \-a /dev/null run
.Ve
.PP
In the stap window you will see a large amount of output, with the time
taken for each step shown (microseconds in parenthesis).  For example:
.PP
.Vb 9
\& xxxx (+0):     guestfs_create 
\& xxxx (+29):    guestfs_set_pgroup g=0x17a9de0 pgroup=0x1
\& xxxx (+9):     guestfs_add_drive_opts_argv g=0x17a9de0 [...]
\& xxxx (+8):     guestfs_safe_strdup g=0x17a9de0 str=0x7f8a153bed5d
\& xxxx (+19):    guestfs_safe_malloc g=0x17a9de0 nbytes=0x38
\& xxxx (+5):     guestfs_safe_strdup g=0x17a9de0 str=0x17a9f60
\& xxxx (+10):    guestfs_launch g=0x17a9de0
\& xxxx (+4):     launch_start 
\& [etc]
.Ve
.PP
You will need to consult, and even modify, the source to libguestfs to fully
understand the output.
.SH "DETAILED DEBUGGING USING GDB"
.IX Header "DETAILED DEBUGGING USING GDB"
You can attach to the appliance BIOS/kernel using gdb.  If you know what
you're doing, this can be a useful way to diagnose boot regressions.
.PP
Firstly, you have to change qemu so it runs with the \f(CW\*(C`\-S\*(C'\fR and \f(CW\*(C`\-s\*(C'\fR
options.  These options cause qemu to pause at boot and allow you to attach
a debugger.  Read \fIqemu\fR\|(1) for further information.  Libguestfs invokes
qemu several times (to scan the help output and so on) and you only want the
final invocation of qemu to use these options, so use a qemu wrapper script
like this:
.PP
.Vb 1
\& #!/bin/bash \-
\& 
\& # Set this to point to the real qemu binary.
\& qemu=/usr/bin/qemu\-kvm
\& 
\& if [ "$1" != "\-global" ]; then
\&     # Scanning help output etc.
\&     exec $qemu "$@"
\& else 
\&     # Really running qemu.
\&     exec $qemu \-S \-s "$@"
\& fi
.Ve
.PP
Now run guestfish or another libguestfs tool with the qemu wrapper (see
\&\*(L"\s-1QEMU\s0 \s-1WRAPPERS\s0\*(R" in \fIguestfs\fR\|(3) to understand what this is doing):
.PP
.Vb 1
\& LIBGUESTFS_QEMU=/path/to/qemu\-wrapper guestfish \-a /dev/null \-v run
.Ve
.PP
This should pause just after qemu launches.  In another window, attach to
qemu using gdb:
.PP
.Vb 7
\& $ gdb
\& (gdb) set architecture i8086
\& The target architecture is assumed to be i8086
\& (gdb) target remote :1234
\& Remote debugging using :1234
\& 0x0000fff0 in ?? ()
\& (gdb) cont
.Ve
.PP
At this point you can use standard gdb techniques, eg. hitting \f(CW\*(C`^C\*(C'\fR to
interrupt the boot and \f(CW\*(C`bt\*(C'\fR get a stack trace, setting breakpoints, etc.
Note that when you are past the \s-1BIOS\s0 and into the Linux kernel, you'll want
to change the architecture back to 32 or 64 bit.
.SH "ТАКОЖ ПЕРЕГЛЯНЬТЕ"
.IX Header "ТАКОЖ ПЕРЕГЛЯНЬТЕ"
\&\fIfebootstrap\fR\|(8), \fIfebootstrap\-supermin\-helper\fR\|(8), \fIguestfish\fR\|(1),
\&\fIguestfs\fR\|(3), \fIguestfs\-examples\fR\|(3),
\&\fIlibguestfs\-make\-fixed\-appliance\fR\|(1), \fIstap\fR\|(1), \fIqemu\fR\|(1), \fIgdb\fR\|(1),
http://libguestfs.org/.
.SH "АВТОРИ"
.IX Header "АВТОРИ"
Richard W.M. Jones (\f(CW\*(C`rjones at redhat dot com\*(C'\fR)
.SH "АВТОРСЬКІ ПРАВА"
.IX Header "АВТОРСЬКІ ПРАВА"
Copyright (C) 2012 Red Hat Inc.
.SH "LICENSE"
.IX Header "LICENSE"
.SH "BUGS"
.IX Header "BUGS"
To get a list of bugs against libguestfs, use this link:
https://bugzilla.redhat.com/buglist.cgi?component=libguestfs&product=Virtualization+Tools
.PP
To report a new bug against libguestfs, use this link:
https://bugzilla.redhat.com/enter_bug.cgi?component=libguestfs&product=Virtualization+Tools
.PP
When reporting a bug, please supply:
.IP "\(bu" 4
The version of libguestfs.
.IP "\(bu" 4
Where you got libguestfs (eg. which Linux distro, compiled from source, etc)
.IP "\(bu" 4
Describe the bug accurately and give a way to reproduce it.
.IP "\(bu" 4
Run \fIlibguestfs\-test\-tool\fR\|(1) and paste the \fBcomplete, unedited\fR
output into the bug report.
