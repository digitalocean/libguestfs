.\" Automatically generated by Podwrapper::Man 1.30.2 (Pod::Simple 3.30)
.\"
.\" Standard preamble:
.\" ========================================================================
.de Sp \" Vertical space (when we can't use .PP)
.if t .sp .5v
.if n .sp
..
.de Vb \" Begin verbatim text
.ft CW
.nf
.ne \\$1
..
.de Ve \" End verbatim text
.ft R
.fi
..
.\" Set up some character translations and predefined strings.  \*(-- will
.\" give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
.\" double quote, and \*(R" will give a right double quote.  \*(C+ will
.\" give a nicer C++.  Capital omega is used to do unbreakable dashes and
.\" therefore won't be available.  \*(C` and \*(C' expand to `' in nroff,
.\" nothing in troff, for use with C<>.
.tr \(*W-
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.ie n \{\
.    ds -- \(*W-
.    ds PI pi
.    if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.    if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\"  diablo 12 pitch
.    ds L" ""
.    ds R" ""
.    ds C` ""
.    ds C' ""
'br\}
.el\{\
.    ds -- \|\(em\|
.    ds PI \(*p
.    ds L" ``
.    ds R" ''
.    ds C`
.    ds C'
'br\}
.\"
.\" Escape single quotes in literal strings from groff's Unicode transform.
.ie \n(.g .ds Aq \(aq
.el       .ds Aq '
.\"
.\" If the F register is turned on, we'll generate index entries on stderr for
.\" titles (.TH), headers (.SH), subsections (.SS), items (.Ip), and index
.\" entries marked with X<> in POD.  Of course, you'll have to process the
.\" output yourself in some meaningful fashion.
.\"
.\" Avoid warning from groff about undefined register 'F'.
.de IX
..
.nr rF 0
.if \n(.g .if rF .nr rF 1
.if (\n(rF:(\n(.g==0)) \{
.    if \nF \{
.        de IX
.        tm Index:\\$1\t\\n%\t"\\$2"
..
.        if !\nF==2 \{
.            nr % 0
.            nr F 2
.        \}
.    \}
.\}
.rr rF
.\" ========================================================================
.\"
.IX Title "virt-v2v 1"
.TH virt-v2v 1 "2015-09-08" "libguestfs-1.30.2" "Virtualization Support"
.\" For nroff, turn off justification.  Always turn off hyphenation; it makes
.\" way too many mistakes in technical documents.
.if n .ad l
.nh
.SH "НАЗВА"
.IX Header "НАЗВА"
virt\-v2v \- Convert a guest to use \s-1KVM\s0
.SH "КОРОТКИЙ ОПИС"
.IX Header "КОРОТКИЙ ОПИС"
.Vb 1
\& virt\-v2v \-ic vpx://vcenter.example.com/Datacenter/esxi vmware_guest
\&
\& virt\-v2v \-ic vpx://vcenter.example.com/Datacenter/esxi vmware_guest \e
\&   \-o rhev \-os rhev.nfs:/export_domain \-\-network rhevm
\&
\& virt\-v2v \-i libvirtxml guest\-domain.xml \-o local \-os /var/tmp
\&
\& virt\-v2v \-i disk disk.img \-o local \-os /var/tmp
\&
\& virt\-v2v \-i disk disk.img \-o glance
.Ve
.SH "ОПИС"
.IX Header "ОПИС"
Virt\-v2v converts guests from a foreign hypervisor to run on \s-1KVM. \s0 It can
read Linux and Windows guests running on VMware, Xen, Hyper-V and some other
hypervisors, and convert them to \s-1KVM\s0 managed by libvirt, OpenStack, oVirt,
Red Hat Enterprise Virtualisation (\s-1RHEV\s0) or several other targets.
.PP
There is also a companion front-end called \fIvirt\-p2v\fR\|(1) which comes as an
\&\s-1ISO, CD\s0 or \s-1PXE\s0 image that can be booted on physical machines to virtualize
those machines (physical to virtual, or p2v).
.PP
This manual page documents the rewritten virt\-v2v included in libguestfs
≥ 1.28.
.SH "INPUT AND OUTPUT MODES"
.IX Header "INPUT AND OUTPUT MODES"
.Vb 10
\&                          ┌────────────┐  ┌─────────▶ \-o null
\& \-i disk ────────────┐    │            │ ─┘┌───────▶ \-o local
\& \-i ova  ──────────┐ └──▶ │ virt\-v2v   │ ──┘┌───────▶ \-o qemu
\&                   └────▶ │ conversion │ ───┘┌────────────┐
\& VMware─▶┌────────────┐   │ server     │ ────▶ \-o libvirt │─▶ KVM
\& Xen ───▶│ \-i libvirt ──▶ │            │     │  (default) │
\& ... ───▶│  (default) │   │            │ ──┐ └────────────┘
\&         └────────────┘   │            │ ─┐└──────▶ \-o glance
\& \-i libvirtxml ─────────▶ │            │ ┐└─────────▶ \-o rhev
\&                          └────────────┘ └──────────▶ \-o vdsm
.Ve
.PP
Virt\-v2v has a number of possible input and output modes, selected using the
\&\fI\-i\fR and \fI\-o\fR options.  Only one input and output mode can be selected for
each run of virt\-v2v.
.PP
\&\fI\-i disk\fR is used for reading from local disk images (mainly for testing).
.PP
\&\fI\-i libvirt\fR is used for reading from any libvirt source.  Since libvirt
can connect to many different hypervisors, it is used for reading guests
from VMware, \s-1RHEL 5\s0 Xen and more.  The \fI\-ic\fR option selects the precise
libvirt source.
.PP
\&\fI\-i libvirtxml\fR is used to read from libvirt \s-1XML\s0 files.  This is the method
used by \fIvirt\-p2v\fR\|(1) behind the scenes.
.PP
\&\fI\-i ova\fR is used for reading from a VMware ova source file.
.PP
\&\fI\-o glance\fR is used for writing to OpenStack Glance.
.PP
\&\fI\-o libvirt\fR is used for writing to any libvirt target.  Libvirt can
connect to local or remote \s-1KVM\s0 hypervisors.  The \fI\-oc\fR option selects the
precise libvirt target.
.PP
\&\fI\-o local\fR is used to write to a local disk image with a local libvirt
configuration file (mainly for testing).
.PP
\&\fI\-o qemu\fR writes to a local disk image with a shell script for booting the
guest directly in qemu (mainly for testing).
.PP
\&\fI\-o rhev\fR is used to write to a RHEV-M / oVirt target.  \fI\-o vdsm\fR is only
used when virt\-v2v runs under \s-1VDSM\s0 control.
.SH "ПРИКЛАДИ"
.IX Header "ПРИКЛАДИ"
.SS "Convert from VMware vCenter server to local libvirt"
.IX Subsection "Convert from VMware vCenter server to local libvirt"
You have a VMware vCenter server called \f(CW\*(C`vcenter.example.com\*(C'\fR, a datacenter
called \f(CW\*(C`Datacenter\*(C'\fR, and an ESXi hypervisor called \f(CW\*(C`esxi\*(C'\fR.  You want to
convert a guest called \f(CW\*(C`vmware_guest\*(C'\fR to run locally under libvirt.
.PP
.Vb 1
\& virt\-v2v \-ic vpx://vcenter.example.com/Datacenter/esxi vmware_guest
.Ve
.PP
In this case you will most likely have to run virt\-v2v as \f(CW\*(C`root\*(C'\fR, since it
needs to talk to the system libvirt daemon and copy the guest disks to
\&\fI/var/lib/libvirt/images\fR.
.PP
For more information see \*(L"\s-1INPUT FROM VMWARE VCENTER SERVER\*(R"\s0 below.
.SS "Convert from VMware to RHEV\-M/oVirt"
.IX Subsection "Convert from VMware to RHEV-M/oVirt"
This is the same as the previous example, except you want to send the guest
to a RHEV-M Export Storage Domain which is located remotely (over \s-1NFS\s0) at
\&\f(CW\*(C`rhev.nfs:/export_domain\*(C'\fR.  If you are unclear about the location of the
Export Storage Domain you should check the settings on your RHEV-M
management console.  Guest network interface(s) are connected to the target
network called \f(CW\*(C`rhevm\*(C'\fR.
.PP
.Vb 2
\& virt\-v2v \-ic vpx://vcenter.example.com/Datacenter/esxi vmware_guest \e
\&   \-o rhev \-os rhev.nfs:/export_domain \-\-network rhevm
.Ve
.PP
In this case the host running virt\-v2v acts as a \fBconversion server\fR.
.PP
Note that after conversion, the guest will appear in the RHEV-M Export
Storage Domain, from where you will need to import it using the RHEV-M user
interface.  (See \*(L"\s-1OUTPUT TO RHEV\*(R"\s0).
.SS "Convert disk image to OpenStack glance"
.IX Subsection "Convert disk image to OpenStack glance"
Given a disk image from another hypervisor that you want to convert to run
on OpenStack (only KVM-based OpenStack is supported), you can do:
.PP
.Vb 1
\& virt\-v2v \-i disk disk.img \-o glance
.Ve
.PP
To control the name of the image in Glance, use the \fI\-on\fR option.
.SS "Перетворити образ диска на образ диска"
.IX Subsection "Перетворити образ диска на образ диска"
Given a disk image from another hypervisor that you want to convert to run
on \s-1KVM,\s0 you have two options.  The simplest way is to try:
.PP
.Vb 1
\& virt\-v2v \-i disk disk.img \-o local \-os /var/tmp
.Ve
.PP
where virt\-v2v guesses everything about the input \fIdisk.img\fR and (in this
case) writes the converted result to \fI/var/tmp\fR.
.PP
A more complex method is to write some libvirt
\&\s-1XML\s0 describing the input guest (if you
can get the source hypervisor to provide you with libvirt \s-1XML,\s0 then so much
the better).  You can then do:
.PP
.Vb 1
\& virt\-v2v \-i libvirtxml guest\-domain.xml \-o local \-os /var/tmp
.Ve
.PP
Since \fIguest\-domain.xml\fR contains the path(s) to the guest disk image(s)
you do not need to specify the name of the disk image on the command line.
.PP
To convert a local disk image and immediately boot it in local qemu, do:
.PP
.Vb 1
\& virt\-v2v \-i disk disk.img \-o qemu \-os /var/tmp \-\-qemu\-boot
.Ve
.SH "SUPPORT MATRIX"
.IX Header "SUPPORT MATRIX"
.SS "Гіпервізори (вхід)"
.IX Subsection "Гіпервізори (вхід)"
.IP "VMware ESXi" 4
.IX Item "VMware ESXi"
Must be managed by VMware vCenter ≥ 5.0.  Unmanaged, direct input from
ESXi is not supported.
.IP "\s-1OVA\s0 exported from VMware" 4
.IX Item "OVA exported from VMware"
OVAs from other hypervisors will not work.
.IP "\s-1RHEL 5\s0 Xen" 4
.IX Item "RHEL 5 Xen"
.PD 0
.IP "Citrix Xen" 4
.IX Item "Citrix Xen"
.PD
Citrix Xen has not been recently tested.
.IP "Hyper-V" 4
.IX Item "Hyper-V"
Not recently tested.  Requires that you export the disk or use
\&\fIvirt\-p2v\fR\|(1) on Hyper-V.
.IP "Безпосередньо з образів дисків" 4
.IX Item "Безпосередньо з образів дисків"
Only disk images exported from supported hypervisors, and using container
formats supported by qemu.
.IP "Physical machines" 4
.IX Item "Physical machines"
За допомогою інструмента \fIvirt\-p2v\fR\|(1).
.SS "Hypervisors (Output)"
.IX Subsection "Hypervisors (Output)"
\&\s-1QEMU\s0 and \s-1KVM\s0 only.
.SS "Virtualization management systems (Output)"
.IX Subsection "Virtualization management systems (Output)"
.IP "OpenStack Glance" 4
.IX Item "OpenStack Glance"
.PD 0
.IP "Red Hat Enterprise Virtualization (\s-1RHEV\s0) 2.2 and up" 4
.IX Item "Red Hat Enterprise Virtualization (RHEV) 2.2 and up"
.IP "Локальна libvirt" 4
.IX Item "Локальна libvirt"
.PD
And hence \fIvirsh\fR\|(1), \fIvirt\-manager\fR\|(1), and similar tools.
.IP "Локальний диск" 4
.IX Item "Локальний диск"
.SS "Гості"
.IX Subsection "Гості"
.PD 0
.IP "Red Hat Enterprise Linux 3, 4, 5, 6, 7" 4
.IX Item "Red Hat Enterprise Linux 3, 4, 5, 6, 7"
.IP "CentOS 3, 4, 5, 6, 7" 4
.IX Item "CentOS 3, 4, 5, 6, 7"
.IP "Scientific Linux 3, 4, 5, 6, 7" 4
.IX Item "Scientific Linux 3, 4, 5, 6, 7"
.IP "Oracle Linux" 4
.IX Item "Oracle Linux"
.IP "Fedora" 4
.IX Item "Fedora"
.IP "\s-1SLES 10\s0 and up" 4
.IX Item "SLES 10 and up"
.IP "OpenSUSE 10 and up" 4
.IX Item "OpenSUSE 10 and up"
.IP "Windows \s-1XP\s0 to Windows 8.1 / Windows Server 2012 R2" 4
.IX Item "Windows XP to Windows 8.1 / Windows Server 2012 R2"
.PD
We use Windows internal version numbers, see
https://en.wikipedia.org/wiki/List_of_Microsoft_Windows_versions
.Sp
Currently \s-1NT 5.2\s0 to \s-1NT 6.3\s0 are supported.
.Sp
See \*(L"\s-1WINDOWS\*(R"\s0 below for additional notes on converting Windows guests.
.SS "Guest firmware"
.IX Subsection "Guest firmware"
\&\s-1BIOS\s0 or \s-1UEFI\s0 for all guest types (but see \*(L"\s-1UEFI\*(R"\s0 below).
.SH "ПАРАМЕТРИ"
.IX Header "ПАРАМЕТРИ"
.IP "\fB\-\-help\fR" 4
.IX Item "--help"
Показати довідкове повідомлення.
.IP "\fB\-b\fR ..." 4
.IX Item "-b ..."
.PD 0
.IP "\fB\-\-bridge\fR ..." 4
.IX Item "--bridge ..."
.PD
Див. \fI\-\-network\fR нижче.
.IP "\fB\-\-debug\-gc\fR" 4
.IX Item "--debug-gc"
Debug garbage collection and memory allocation.  This is only useful when
debugging memory problems in virt\-v2v or the OCaml libguestfs bindings.
.IP "\fB\-\-debug\-overlays\fR" 4
.IX Item "--debug-overlays"
Save the overlay file(s) created during conversion.  This option is only
used for debugging virt\-v2v and may be removed in a future version.
.IP "\fB\-i disk\fR" 4
.IX Item "-i disk"
Встановити метод введення \fIdisk\fR.
.Sp
In this mode you can read a virtual machine disk image with no metadata.
virt\-v2v tries to guess the best default metadata.  This is usually adequate
but you can get finer control (eg. of memory and vCPUs) by using \fI\-i
libvirtxml\fR instead.  Only guests that use a single disk can be imported
this way.
.IP "\fB\-i libvirt\fR" 4
.IX Item "-i libvirt"
Встановити метод введення \fIlibvirt\fR. Цей метод є типовим.
.Sp
In this mode you have to specify a libvirt guest name or \s-1UUID\s0 on the command
line.  You may also specify a libvirt connection \s-1URI \s0(see \fI\-ic\fR).
.IP "\fB\-i libvirtxml\fR" 4
.IX Item "-i libvirtxml"
Встановити метод введення \fIlibvirtxml\fR.
.Sp
In this mode you have to pass a libvirt \s-1XML\s0 file on the command line.  This
file is read in order to get metadata about the source guest (such as its
name, amount of memory), and also to locate the input disks.  See \*(L"\s-1MINIMAL
XML FOR\s0 \-i libvirtxml \s-1OPTION\*(R"\s0 below.
.IP "\fB\-i local\fR" 4
.IX Item "-i local"
Те саме, що і \fI\-i disk\fR.
.IP "\fB\-i ova\fR" 4
.IX Item "-i ova"
Встановити метод введення \fIova\fR.
.Sp
In this mode you can read a VMware ova file.  Virt\-v2v will read the ova
manifest file and check the vmdk volumes for validity (checksums)  as well
as analyzing the ovf file, and then convert the guest.  See \*(L"\s-1INPUT FROM
VMWARE OVA\*(R"\s0 below
.IP "\fB\-ic\fR адреса_libvirt" 4
.IX Item "-ic адреса_libvirt"
Specify a libvirt connection \s-1URI\s0 to use when reading the guest.  This is
only used when \fI\-i libvirt\fR.
.Sp
Only local libvirt connections, VMware vCenter connections, or \s-1RHEL 5\s0 Xen
remote connections can be used.  Other remote libvirt connections will not
work in general.
.Sp
See also \*(L"\s-1INPUT FROM VMWARE VCENTER SERVER\*(R"\s0, \*(L"\s-1INPUT FROM RHEL 5 XEN\*(R"\s0
below.
.IP "\fB\-if\fR формат" 4
.IX Item "-if формат"
For \fI\-i disk\fR only, this specifies the format of the input disk image.  For
other input methods you should specify the input format in the metadata.
.IP "\fB\-\-machine\-readable\fR" 4
.IX Item "--machine-readable"
This option is used to make the output more machine friendly when being
parsed by other programs.  See \*(L"\s-1MACHINE READABLE OUTPUT\*(R"\s0 below.
.IP "\fB\-n\fR введення:виведення" 4
.IX Item "-n введення:виведення"
.PD 0
.IP "\fB\-n\fR виведення" 4
.IX Item "-n виведення"
.IP "\fB\-\-network\fR введення:виведення" 4
.IX Item "--network введення:виведення"
.IP "\fB\-\-network\fR виведення" 4
.IX Item "--network виведення"
.IP "\fB\-b\fR введення:виведення" 4
.IX Item "-b введення:виведення"
.IP "\fB\-b\fR виведення" 4
.IX Item "-b виведення"
.IP "\fB\-\-bridge\fR введення:виведення" 4
.IX Item "--bridge введення:виведення"
.IP "\fB\-\-bridge\fR виведення" 4
.IX Item "--bridge виведення"
.PD
Map network (or bridge) called \f(CW\*(C`in\*(C'\fR to network (or bridge) called \f(CW\*(C`out\*(C'\fR.
If no \f(CW\*(C`in:\*(C'\fR prefix is given, all other networks (or bridges)  are mapped to
\&\f(CW\*(C`out\*(C'\fR.
.Sp
див. \*(L"МЕРЕЖІ І МІСТКИ\*(R" нижче.
.IP "\fB\-\-no\-copy\fR" 4
.IX Item "--no-copy"
Don't copy the disks.  Instead, conversion is performed (and thrown away),
and metadata is written, but no disks are created.  See also discussion of
\&\fI\-o null\fR below.
.Sp
This is useful in two cases: Either you want to test if conversion is likely
to succeed, without the long copying process.  Or you are only interested in
looking at the metadata.
.Sp
This option is not compatible with \fI\-o libvirt\fR since it would create a
faulty guest (one with no disks).
.Sp
This option is not compatible with \fI\-o glance\fR for technical reasons.
.IP "\fB\-\-no\-trim all\fR" 4
.IX Item "--no-trim all"
.PD 0
.IP "\fB\-\-no\-trim\fR mp[,mp...]" 4
.IX Item "--no-trim mp[,mp...]"
.PD
By default virt\-v2v runs \fIfstrim\fR\|(8) to reduce the amount of data that
needs to be copied.  This is known to break some buggy bootloaders causing
boot failures after conversion (see for example
https://bugzilla.redhat.com/show_bug.cgi?id=1141145#c27).
.Sp
You can use \fI\-\-no\-trim all\fR to disable all trimming.  Note this will
greatly increase the amount of data that has to be copied and can make
virt\-v2v run much more slowly.
.Sp
You can also disable trimming on selected filesystems only (specified by a
comma-separated list of their mount point(s) in the guest).  Typically you
would use \fI\-\-no\-trim /boot\fR to work around the grub bug mentioned above.
.Sp
You can also disable trimming on partitions using the libguestfs naming
scheme for devices, eg: \fI\-\-no\-trim /dev/sdb2\fR means do not trim the second
partition on the second block device.  Use \fIvirt\-filesystems\fR\|(1) to list
filesystem names in a guest.
.IP "\fB\-o disk\fR" 4
.IX Item "-o disk"
Те саме, що і \fI\-o local\fR.
.IP "\fB\-o glance\fR" 4
.IX Item "-o glance"
Set the output method to OpenStack Glance.  In this mode the converted guest
is uploaded to Glance.  You can control the image name by setting the \fI\-on\fR
option.
.IP "\fB\-o libvirt\fR" 4
.IX Item "-o libvirt"
Set the output method to \fIlibvirt\fR.  This is the default.
.Sp
In this mode, the converted guest is created as a libvirt guest.  You may
also specify a libvirt connection \s-1URI \s0(see \fI\-oc\fR).
.Sp
Див. \*(L"ВИВЕДЕННЯ ДО \s-1LIBVIRT\*(R"\s0 нижче.
.IP "\fB\-o local\fR" 4
.IX Item "-o local"
Встановити метод виведення до \fIlocal\fR.
.Sp
In this mode, the converted guest is written to a local directory specified
by \fI\-os /dir\fR (the directory must exist).  The converted guest's disks are
written as:
.Sp
.Vb 3
\& /dir/name\-sda
\& /dir/name\-sdb
\& [тощо]
.Ve
.Sp
and a libvirt \s-1XML\s0 file is created containing guest metadata:
.Sp
.Vb 1
\& /каталог/назва.xml
.Ve
.Sp
де \f(CW\*(C`назва\*(C'\fR — назва гостьової системи.
.IP "\fB\-o null\fR" 4
.IX Item "-o null"
Встановити метод виведення до \fInull\fR.
.Sp
The guest is converted and copied (unless you also specify \fI\-\-no\-copy\fR),
but the results are thrown away and no metadata is written.
.IP "\fB\-o ovirt\fR" 4
.IX Item "-o ovirt"
Це те саме, що і \fI\-o rhev\fR.
.IP "\fB\-o qemu\fR" 4
.IX Item "-o qemu"
Встановити метод виведення до \fIqemu\fR.
.Sp
This is similar to \fI\-o local\fR, except that a shell script is written which
you can use to boot the guest in qemu.  The converted disks and shell script
are written to the directory specified by \fI\-os\fR.
.Sp
When using this output mode, you can also specify the \fI\-\-qemu\-boot\fR option
which boots the guest under qemu immediately.
.IP "\fB\-o rhev\fR" 4
.IX Item "-o rhev"
Встановити метод виведення до \fIrhev\fR.
.Sp
The converted guest is written to a \s-1RHEV\s0 Export Storage Domain.  The \fI\-os\fR
parameter must also be used to specify the location of the Export Storage
Domain.  Note this does not actually import the guest into \s-1RHEV. \s0 You have
to do that manually later using the \s-1UI.\s0
.Sp
Див. \*(L"ВИВЕДЕННЯ ДО \s-1RHEV\*(R"\s0 нижче.
.IP "\fB\-o vdsm\fR" 4
.IX Item "-o vdsm"
Встановити метод виведення до \fIvdsm\fR.
.Sp
This mode is similar to \fI\-o rhev\fR, but the full path to the data domain
must be given:
\&\fI/rhev/data\-center/<data\-center\-uuid>/<data\-domain\-uuid>\fR.
This mode is only used when virt\-v2v runs under \s-1VDSM\s0 control.
.IP "\fB\-oa sparse\fR" 4
.IX Item "-oa sparse"
.PD 0
.IP "\fB\-oa preallocated\fR" 4
.IX Item "-oa preallocated"
.PD
Set the output file allocation mode.  The default is \f(CW\*(C`sparse\*(C'\fR.
.IP "\fB\-oc\fR адреса_libvirt" 4
.IX Item "-oc адреса_libvirt"
Specify a libvirt connection to use when writing the converted guest.  This
is only used when \fI\-o libvirt\fR.  See \*(L"\s-1OUTPUT TO LIBVIRT\*(R"\s0 below.
.Sp
Only local libvirt connections can be used.  Remote libvirt connections will
not work.
.IP "\fB\-of\fR формат" 4
.IX Item "-of формат"
When converting the guest, convert the disks to the given format.
.Sp
If not specified, then the input format is used.
.IP "\fB\-on\fR назва" 4
.IX Item "-on назва"
Rename the guest when converting it.  If this option is not used then the
output name is the same as the input name.
.IP "\fB\-os\fR сховище" 4
.IX Item "-os сховище"
The location of the storage for the converted guest.
.Sp
For \fI\-o libvirt\fR, this is a libvirt directory pool (see \f(CW\*(C`virsh pool\-list\*(C'\fR) or pool \s-1UUID.\s0
.Sp
For \fI\-o local\fR and \fI\-o qemu\fR, this is a directory name.  The directory
must exist.
.Sp
For \fI\-o rhev\fR, this can be an \s-1NFS\s0 path of the Export Storage Domain of the
form \f(CW\*(C`<host>:<path>\*(C'\fR, eg:
.Sp
.Vb 1
\& rhev\-storage.example.com:/rhev/export
.Ve
.Sp
The \s-1NFS\s0 export must be mountable and writable by the user and host running
virt\-v2v, since the virt\-v2v program has to actually mount it when it runs.
So you probably have to run virt\-v2v as \f(CW\*(C`root\*(C'\fR.
.Sp
\&\fBOr:\fR You can mount the Export Storage Domain yourself, and point \fI\-os\fR to
the mountpoint.  Note that virt\-v2v will still need to write to this remote
directory, so virt\-v2v will still need to run as \f(CW\*(C`root\*(C'\fR.
.Sp
You will get an error if virt\-v2v is unable to mount/write to the Export
Storage Domain.
.IP "\fB\-\-password\-file\fR файл" 4
.IX Item "--password-file файл"
Instead of asking for password(s) interactively, pass the password through a
file.  Note the file should contain the whole password, \fBwithout any
trailing newline\fR, and for security the file should have mode \f(CW0600\fR so
that others cannot read it.
.IP "\fB\-\-print\-source\fR" 4
.IX Item "--print-source"
Print information about the source guest and stop.  This option is useful
when you are setting up network and bridge maps.  See \*(L"\s-1NETWORKS AND
BRIDGES\*(R"\s0.
.IP "\fB\-\-qemu\-boot\fR" 4
.IX Item "--qemu-boot"
When using \fI\-o qemu\fR only, this boots the guest immediately after virt\-v2v
finishes.
.IP "\fB\-q\fR" 4
.IX Item "-q"
.PD 0
.IP "\fB\-\-quiet\fR" 4
.IX Item "--quiet"
.PD
This disables progress bars and other unnecessary output.
.IP "\fB\-\-root ask\fR" 4
.IX Item "--root ask"
.PD 0
.IP "\fB\-\-root single\fR" 4
.IX Item "--root single"
.IP "\fB\-\-root first\fR" 4
.IX Item "--root first"
.IP "\fB\-\-root\fR /dev/sdX" 4
.IX Item "--root /dev/sdX"
.IP "\fB\-\-root\fR /dev/VG/LV" 4
.IX Item "--root /dev/VG/LV"
.PD
Choose the root filesystem to be converted.
.Sp
In the case where the virtual machine is dual-boot or multi-boot, or where
the \s-1VM\s0 has other filesystems that look like operating systems, this option
can be used to select the root filesystem (a.k.a. \f(CW\*(C`C:\*(C'\fR drive or \fI/\fR) of
the operating system that is to be converted.  The Windows Recovery Console,
certain attached \s-1DVD\s0 drives, and bugs in libguestfs inspection heuristics,
can make a guest look like a multi-boot operating system.
.Sp
The default in virt\-v2v ≤ 0.7.1 was \fI\-\-root single\fR, which causes
virt\-v2v to die if a multi-boot operating system is found.
.Sp
Since virt\-v2v ≥ 0.7.2 the default is now \fI\-\-root ask\fR: If the \s-1VM\s0 is
found to be multi-boot, then virt\-v2v will stop and list the possible root
filesystems and ask the user which to use.  This requires that virt\-v2v is
run interactively.
.Sp
\&\fI\-\-root first\fR means to choose the first root device in the case of a
multi-boot operating system.  Since this is a heuristic, it may sometimes
choose the wrong one.
.Sp
You can also name a specific root device, eg. \fI\-\-root /dev/sda2\fR would
mean to use the second partition on the first hard drive.  If the named root
device does not exist or was not detected as a root device, then virt\-v2v
will fail.
.Sp
Note that there is a bug in grub which prevents it from successfully booting
a multiboot system if VirtIO is enabled.  Grub is only able to boot an
operating system from the first VirtIO disk.  Specifically, \fI/boot\fR must be
on the first VirtIO disk, and it cannot chainload an \s-1OS\s0 which is not in the
first VirtIO disk.
.IP "\fB\-\-vdsm\-image\-uuid\fR \s-1UUID\s0" 4
.IX Item "--vdsm-image-uuid UUID"
.PD 0
.IP "\fB\-\-vdsm\-vol\-uuid\fR \s-1UUID\s0" 4
.IX Item "--vdsm-vol-uuid UUID"
.IP "\fB\-\-vdsm\-vm\-uuid\fR \s-1UUID\s0" 4
.IX Item "--vdsm-vm-uuid UUID"
.IP "\fB\-\-vdsm\-ovf\-output\fR" 4
.IX Item "--vdsm-ovf-output"
.PD
Normally the \s-1RHEV\s0 output mode chooses random UUIDs for the target guest.
However \s-1VDSM\s0 needs to control the UUIDs and passes these parameters when
virt\-v2v runs under \s-1VDSM\s0 control.  The parameters control:
.RS 4
.IP "\(bu" 4
the image directory of each guest disk (\fI\-\-vdsm\-image\-uuid\fR) (this option
is passed once for each guest disk)
.IP "\(bu" 4
UUIDs for each guest disk (\fI\-\-vdsm\-vol\-uuid\fR) (this option is passed once
for each guest disk)
.IP "\(bu" 4
the \s-1OVF\s0 file name (\fI\-\-vdsm\-vm\-uuid\fR).
.IP "\(bu" 4
the \s-1OVF\s0 output directory (default current directory) (\fI\-\-vdsm\-ovf\-output\fR).
.RE
.RS 4
.Sp
The format of UUIDs is: \f(CW\*(C`12345678\-1234\-1234\-1234\-123456789abc\*(C'\fR (each hex
digit can be \f(CW\*(C`0\-9\*(C'\fR or \f(CW\*(C`a\-f\*(C'\fR), conforming to \s-1OSF DCE 1.1.\s0
.Sp
Ці параметри можна використовувати лише з \fI\-o vdsm\fR.
.RE
.IP "\fB\-v\fR" 4
.IX Item "-v"
.PD 0
.IP "\fB\-\-verbose\fR" 4
.IX Item "--verbose"
.PD
Увімкнути докладний показ повідомлень з метою діагностики.
.IP "\fB\-V\fR" 4
.IX Item "-V"
.PD 0
.IP "\fB\-\-version\fR" 4
.IX Item "--version"
.PD
Показати дані щодо версії і завершити роботу.
.IP "\fB\-\-vmtype desktop\fR" 4
.IX Item "--vmtype desktop"
.PD 0
.IP "\fB\-\-vmtype server\fR" 4
.IX Item "--vmtype server"
.PD
For the \fI\-o rhev\fR or \fI\-o vdsm\fR targets only, specify the type of guest.
You can set this to \f(CW\*(C`desktop\*(C'\fR or \f(CW\*(C`server\*(C'\fR.  If the option is not given,
then a suitable default is chosen based on the detected guest operating
system.
.IP "\fB\-x\fR" 4
.IX Item "-x"
Увімкнути трасування викликів програмного інтерфейсу libguestfs.
.SH "ПАРАВІРТУАЛІЗОВАНІ ГОСТЬОВІ СИСТЕМИ XEN"
.IX Header "ПАРАВІРТУАЛІЗОВАНІ ГОСТЬОВІ СИСТЕМИ XEN"
Older versions of virt\-v2v could turn a Xen paravirtualized (\s-1PV\s0) guest into
a \s-1KVM\s0 guest by installing a new kernel.  This version of virt\-v2v does
\&\fInot\fR attempt to install any new kernels.  Instead it will give you an
error if there are \fIonly\fR Xen \s-1PV\s0 kernels available.
.PP
Therefore before conversion you should check that a regular kernel is
installed.  For some older Linux distributions, this means installing a
kernel from the table below:
.PP
.Vb 1
\& RHEL 3         (Does not apply, as there was no Xen PV kernel)
\& 
\& RHEL 4         i686 with > 10GB of RAM: install \*(Aqkernel\-hugemem\*(Aq
\&                i686 SMP: install \*(Aqkernel\-smp\*(Aq
\&                other i686: install \*(Aqkernel\*(Aq
\&                x86\-64 SMP with > 8 CPUs: install \*(Aqkernel\-largesmp\*(Aq
\&                x86\-64 SMP: install \*(Aqkernel\-smp\*(Aq
\&                other x86\-64: install \*(Aqkernel\*(Aq
\& 
\& RHEL 5         i686: install \*(Aqkernel\-PAE\*(Aq
\&                x86\-64: install \*(Aqkernel\*(Aq
\& 
\& SLES 10        i586 with > 10GB of RAM: install \*(Aqkernel\-bigsmp\*(Aq
\&                i586 SMP: install \*(Aqkernel\-smp\*(Aq
\&                other i586: install \*(Aqkernel\-default\*(Aq
\&                x86\-64 SMP: install \*(Aqkernel\-smp\*(Aq
\&                other x86\-64: install \*(Aqkernel\-default\*(Aq
\& 
\& SLES 11+       i586: install \*(Aqkernel\-pae\*(Aq
\&                x86\-64: install \*(Aqkernel\-default\*(Aq
\&
\& Windows        (Does not apply, as there is no Xen PV Windows kernel)
.Ve
.SH "ВМИКАННЯ VIRTIO"
.IX Header "ВМИКАННЯ VIRTIO"
\&\*(L"Virtio\*(R" is the name for a set of drivers which make disk (block device),
network and other guest operations work much faster on \s-1KVM.\s0
.PP
Older versions of virt\-v2v could install these drivers for certain Linux
guests.  This version of virt\-v2v does \fInot\fR attempt to install new Linux
kernels or drivers, but will warn you if they are not installed already.
.PP
In order to enable virtio, and hence improve performance of the guest after
conversion, you should ensure that the \fBminimum\fR versions of packages are
installed \fIbefore\fR conversion, by consulting the table below.
.PP
.Vb 1
\& RHEL 3         No virtio drivers are available
\& 
\& RHEL 4         kernel >= 2.5.9\-89.EL
\&                lvm2 >= 2.02.42\-5.el4
\&                device\-mapper >= 1.02.28\-2.el4
\&                selinux\-policy\-targeted >= 1.17.30\-2.152.el4
\&                policycoreutils >= 1.18.1\-4.13
\& 
\& RHEL 5         ядро >= 2.6.18\-128.el5
\&                lvm2 >= 2.02.40\-6.el5
\&                selinux\-policy\-targeted >= 2.4.6\-203.el5
\& 
\& RHEL 6+        усі версії підтримують virtio
\& 
\& Fedora         усі версії підтримують virtio
\& 
\& SLES 11+       усі версії підтримують virtio
\& 
\& SLES 10        ядро >= 2.6.16.60\-0.85.1
\& 
\& OpenSUSE 11+   усі версії підтримують virtio
\& 
\& OpenSUSE 10    ядро >= 2.6.25.5\-1.1
\&
\& Windows        Drivers are installed from the directory pointed to by
\&                "VIRTIO_WIN" environment variable
\&                (/usr/share/virtio\-win by default) if present
.Ve
.SH "RHEL 4"
.IX Header "RHEL 4"
.SS "SELinux relabel appears to hang forever"
.IX Subsection "SELinux relabel appears to hang forever"
In \s-1RHEL\s0 ≤ 4.7 there was a bug which causes SELinux relabelling to appear
to hang forever at:
.PP
.Vb 4
\& *** Warning \-\- SELinux relabel is required. ***
\& *** Disabling security enforcement.         ***
\& *** Relabeling could take a very long time, ***
\& *** depending on file system size.          ***
.Ve
.PP
In reality it is waiting for you to press a key (but there is no visual
indication of this).  You can either hit the \f(CW\*(C`[Return]\*(C'\fR key, at which point
the guest will finish relabelling and reboot, or you can install
policycoreutils ≥ 1.18.1\-4.13 before starting the v2v conversion.  See
also https://bugzilla.redhat.com/show_bug.cgi?id=244636
.SH "WINDOWS"
.IX Header "WINDOWS"
.SS "Boot failure: 0x0000007B"
.IX Subsection "Boot failure: 0x0000007B"
This boot failure is caused by Windows being unable to find or load the
right disk driver (eg. \fIviostor.sys\fR).  If you experience this error, here
are some things to check:
.IP "\(bu" 4
First ensure that the guest boots on the source hypervisor before
conversion.
.IP "\(bu" 4
Check you have the Windows virtio drivers available in
\&\fI/usr/share/virtio\-win\fR, and that virt\-v2v did not print any warning about
not being able to install virtio drivers.
.Sp
On Red Hat Enterprise Linux 7, you will need to install the signed
drivers available in the \f(CW\*(C`virtio\-win\*(C'\fR package.  If you do not have access
to the signed drivers, then you will probably need to disable driver signing
in the boot menus.
.IP "\(bu" 4
Check that you are presenting a virtio-blk interface (\fBnot\fR virtio-scsi and
\&\fBnot\fR ide) to the guest.  On the qemu/KVM command line you should see
something similar to this:
.Sp
.Vb 1
\& ... \-drive file=windows\-sda,if=virtio ...
.Ve
.Sp
In libvirt \s-1XML,\s0 you should see:
.Sp
.Vb 1
\& <target dev=\*(Aqvda\*(Aq bus=\*(Aqvirtio\*(Aq/>
.Ve
.IP "\(bu" 4
Check that Windows Group Policy does not prevent the driver from being
installed or used.  Try deleting Windows Group Policy before conversion.
.IP "\(bu" 4
Check there is no anti-virus or other software which implements Group
Policy-like prohibitions on installing or using new drivers.
.IP "\(bu" 4
Enable boot debugging and check the \fIviostor.sys\fR driver is being loaded.
.SS "OpenStack and Windows reactivation"
.IX Subsection "OpenStack and Windows reactivation"
OpenStack does not offer stable device / \s-1PCI\s0 addresses to guests.  Every
time it creates or starts a guest, it regenerates the libvirt \s-1XML\s0 for that
guest from scratch.  The libvirt \s-1XML\s0 will have no <address> fields.
Libvirt will then assign addresses to devices, in a predictable manner.
Addresses may change if any of the following are true:
.IP "\(bu" 4
A new disk or network device has been added or removed from the guest.
.IP "\(bu" 4
The version of OpenStack or (possibly) libvirt has changed.
.PP
Because Windows does not like \*(L"hardware\*(R" changes of this kind, it may
trigger Windows reactivation.
.PP
This can also prevent booting with a 7B error [see previous section] if the
guest has group policy containing \f(CW\*(C`Device Installation Restrictions\*(C'\fR.
.SH "UEFI"
.IX Header "UEFI"
VMware allows you to present \s-1UEFI\s0 firmware to guests (instead of the
ordinary \s-1PC BIOS\s0).  Virt\-v2v can convert these guests, but requires that
\&\s-1UEFI\s0 is supported by the target hypervisor.
.PP
Currently \s-1KVM\s0 supports \s-1OVMF,\s0 a partially open source \s-1UEFI\s0 firmware, and can
run these guests.
.PP
Since \s-1OVMF\s0 support was only recently added to \s-1KVM \s0(in 2014/2015), not all
target environments support \s-1UEFI\s0 guests yet:
.IP "\s-1UEFI\s0 on libvirt, qemu" 4
.IX Item "UEFI on libvirt, qemu"
Supported.  Virt\-v2v will generate the correct libvirt \s-1XML \s0(metadata)
automatically, but note that the same version of \s-1OVMF\s0 must be installed on
the conversion host as is installed on the target hypervisor, else you will
have to adjust paths in the metadata.
.IP "\s-1UEFI\s0 on OpenStack" 4
.IX Item "UEFI on OpenStack"
Not supported.
.IP "\s-1UEFI\s0 on \s-1RHEV\s0" 4
.IX Item "UEFI on RHEV"
Not supported.
.SH "МЕРЕЖІ І МІСТКИ"
.IX Header "МЕРЕЖІ І МІСТКИ"
Guests are usually connected to one or more networks, and when converted to
the target hypervisor you usually want to reconnect those networks at the
destination.  The options \fI\-\-network\fR and \fI\-\-bridge\fR allow you to do that.
.PP
If you are unsure of what networks and bridges are in use on the source
hypervisor, then you can examine the source metadata (libvirt \s-1XML,\s0 vCenter
information, etc.).  Or you can run virt\-v2v with the \fI\-\-print\-source\fR
option which causes virt\-v2v to print out the information it has about the
guest on the source and then exit.
.PP
In the \fI\-\-print\-source\fR output you will see a section showing the guest's
Network Interface Cards (NICs):
.PP
.Vb 4
\& $ virt\-v2v [\-i ...] \-\-print\-source name
\& [...]
\& NICs:
\&     Network "default" mac: 52:54:00:d0:cf:0e
.Ve
.PP
This is typical of a libvirt guest: It has a single network interface
connected to a network called \f(CW\*(C`default\*(C'\fR.
.PP
To map a specific network to a target network, for example \f(CW\*(C`default\*(C'\fR on the
source to \f(CW\*(C`rhevm\*(C'\fR on the target, use:
.PP
.Vb 1
\& virt\-v2v [...] \-\-network default:rhevm
.Ve
.PP
To map every network to a target network, use:
.PP
.Vb 1
\& virt\-v2v [...] \-\-network rhevm
.Ve
.PP
Bridges are handled in the same way, but you have to use the \fI\-\-bridge\fR
option instead.  For example:
.PP
.Vb 4
\& $ virt\-v2v [\-i ...] \-\-print\-source name
\& [...]
\& NICs:
\&     Bridge "br0"
\& 
\& $ virt\-v2v [...] \-\-bridge br0:targetbr
.Ve
.SH "INPUT FROM VMWARE VCENTER SERVER"
.IX Header "INPUT FROM VMWARE VCENTER SERVER"
Virt\-v2v is able to import guests from VMware vCenter Server.
.PP
vCenter ≥ 5.0 is required.  Virt\-v2v \fBcannot\fR import guests directly
from an ESXi hypervisor.  If you don't have vCenter, using \s-1OVA\s0 is
recommended instead (see \*(L"\s-1INPUT FROM VMWARE OVA\*(R"\s0 below).
.PP
Virt\-v2v uses libvirt for access to vCenter, and therefore the input mode
should be \fI\-i libvirt\fR.  As this is the default, you don't need to specify
it on the command line.
.SS "\s-1VCENTER: REMOVE VMWARE TOOLS FROM WINDOWS GUESTS\s0"
.IX Subsection "VCENTER: REMOVE VMWARE TOOLS FROM WINDOWS GUESTS"
For Windows guests, you should remove VMware tools before conversion.
Although this is not strictly necessary, and the guest will still be able to
run, if you don't do this then the converted guest will complain on every
boot.  The tools cannot be removed after conversion because the uninstaller
checks if it is running on VMware and refuses to start (which is also the
reason that virt\-v2v cannot remove them).
.PP
This is not necessary for Linux guests, as virt\-v2v is able to remove VMware
tools.
.SS "\s-1VCENTER: URI\s0"
.IX Subsection "VCENTER: URI"
The libvirt \s-1URI\s0 of a vCenter server looks something like this:
.PP
.Vb 1
\& vpx://користувач@сервер/Datacenter/esxi
.Ve
.PP
де
.ie n .IP """користувач@""" 4
.el .IP "\f(CWкористувач@\fR" 4
.IX Item "користувач@"
is the (optional, but recommended) user to connect as.
.Sp
If the username contains a backslash (eg. \f(CW\*(C`DOMAIN\eUSER\*(C'\fR) then you will need
to URI-escape that character using \f(CW%5c\fR: \f(CW\*(C`DOMAIN%5cUSER\*(C'\fR (5c is the
hexadecimal \s-1ASCII\s0 code for backslash.)  Other punctuation may also have to
be escaped.
.ie n .IP """server""" 4
.el .IP "\f(CWserver\fR" 4
.IX Item "server"
is the vCenter Server (\fInot\fR hypervisor).
.ie n .IP """Datacenter""" 4
.el .IP "\f(CWDatacenter\fR" 4
.IX Item "Datacenter"
is the name of the datacenter.
.Sp
If the name contains a space, replace it with the URI-escape code \f(CW%20\fR.
.ie n .IP """esxi""" 4
.el .IP "\f(CWesxi\fR" 4
.IX Item "esxi"
is the name of the ESXi hypervisor running the guest.
.PP
If the VMware deployment is using folders, then these may need to be added
to the \s-1URI,\s0 eg:
.PP
.Vb 1
\& vpx://user@server/Folder/Datacenter/esxi
.Ve
.PP
If the deployment uses a cluster before the hostname, then you may need to
remove it, ie. change this:
.PP
.Vb 1
\& vpx://user@server/Folder/Datacenter/Cluster/esxi
.Ve
.PP
to this:
.PP
.Vb 1
\& vpx://user@server/Folder/Datacenter/esxi
.Ve
.PP
Virt\-v2v needs to calculate the \f(CW\*(C`dcPath\*(C'\fR parameter from the \s-1URI,\s0 and it
does this by removing the final \f(CW\*(C`/esxi\*(C'\fR element, so in the above example
\&\f(CW\*(C`dcPath=Folder/Datacenter\*(C'\fR.
.PP
For full details of libvirt URIs, see: http://libvirt.org/drvesx.html
.PP
Typical errors from libvirt / virsh when the \s-1URI\s0 is wrong include:
.IP "\(bu" 4
Could not find datacenter specified in [...]
.IP "\(bu" 4
Could not find compute resource specified in [...]
.IP "\(bu" 4
Path [...] does not specify a compute resource
.IP "\(bu" 4
Path [...] does not specify a host system
.IP "\(bu" 4
Could not find host system specified in [...]
.SS "\s-1VCENTER: TEST LIBVIRT CONNECTION TO VCENTER\s0"
.IX Subsection "VCENTER: TEST LIBVIRT CONNECTION TO VCENTER"
Use the \fIvirsh\fR\|(1) command to list the guests on the vCenter Server like
this:
.PP
.Vb 2
\& $ virsh \-c \*(Aqvpx://root@vcenter.example.com/Datacenter/esxi\*(Aq list \-\-all
\& Enter root\*(Aqs password for vcenter.example.com: ***
\& 
\&  Id    Name                           State
\& \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
\&  \-     Fedora 20                      shut off
\&  \-     Windows 2003                   shut off
.Ve
.PP
If you get an error \*(L"Peer certificate cannot be authenticated with given \s-1CA\s0
certificates\*(R" or similar, then you can either import the vCenter host's
certificate, or bypass signature verification by adding the \f(CW\*(C`?no_verify=1\*(C'\fR
flag:
.PP
.Vb 1
\& $ virsh \-c \*(Aqvpx://root@vcenter.example.com/Datacenter/esxi?no_verify=1\*(Aq list \-\-all
.Ve
.PP
You should also try dumping the metadata from any guest on your server, like
this:
.PP
.Vb 5
\& $ virsh \-c \*(Aqvpx://root@vcenter.example.com/Datacenter/esxi\*(Aq dumpxml "Windows 2003"
\& <domain type=\*(Aqvmware\*(Aq>
\&   <name>Windows 2003</name>
\&   [...]
\& </domain>
.Ve
.PP
\&\fBIf the above commands do not work, then virt\-v2v is not going to work
either\fR.  Fix your libvirt configuration and/or your VMware vCenter Server
before continuing.
.SS "\s-1VCENTER: IMPORTING A GUEST\s0"
.IX Subsection "VCENTER: IMPORTING A GUEST"
To import a particular guest from vCenter Server, do:
.PP
.Vb 3
\& $ virt\-v2v \-ic \*(Aqvpx://root@vcenter.example.com/Datacenter/esxi?no_verify=1\*(Aq \e
\&   "Windows 2003" \e
\&   \-o local \-os /var/tmp
.Ve
.PP
where \f(CW\*(C`Windows 2003\*(C'\fR is the name of the guest (which must be shut down).
.PP
Note that you may be asked for the vCenter password \fItwice\fR.  This happens
once because libvirt needs it, and a second time because virt\-v2v itself
connects directly to the server.  Use \fI\-\-password\-file\fR to supply a
password via a file.
.PP
In this case the output flags are set to write the converted guest to a
temporary directory as this is just an example, but you can also write to
libvirt or any other supported target.
.SH "INPUT FROM VMWARE OVA"
.IX Header "INPUT FROM VMWARE OVA"
Virt\-v2v is able to import guests from VMware's \s-1OVA \s0(Open Virtualization
Appliance) files.  Only OVAs exported from VMware vSphere will work.
.SS "\s-1OVA: REMOVE VMWARE TOOLS FROM WINDOWS GUESTS\s0"
.IX Subsection "OVA: REMOVE VMWARE TOOLS FROM WINDOWS GUESTS"
For Windows guests, you should remove VMware tools before conversion.
Although this is not strictly necessary, and the guest will still be able to
run, if you don't do this then the converted guest will complain on every
boot.  The tools cannot be removed after conversion because the uninstaller
checks if it is running on VMware and refuses to start (which is also the
reason that virt\-v2v cannot remove them).
.PP
This is not necessary for Linux guests, as virt\-v2v is able to remove VMware
tools.
.SS "\s-1OVA: CREATE OVA\s0"
.IX Subsection "OVA: CREATE OVA"
To create an \s-1OVA\s0 in vSphere, use the \*(L"Export \s-1OVF\s0 Template\*(R" option (from the
\&\s-1VM\s0 context menu, or from the File menu).  Either \*(L"Folder of files\*(R" (\s-1OVF\s0) or
\&\*(L"Single file\*(R" (\s-1OVA\s0) will work, but \s-1OVA\s0 is probably easier to deal with.  \s-1OVA\s0
files are really just uncompressed tar files, so you can use commands like
\&\f(CW\*(C`tar tf VM.ova\*(C'\fR to view their contents.
.SS "\s-1OVA: IMPORTING A GUEST\s0"
.IX Subsection "OVA: IMPORTING A GUEST"
To import an \s-1OVA\s0 file called \fI\s-1VM\s0.ova\fR, do;
.PP
.Vb 1
\& $ virt\-v2v \-i ova VM.ova \-o local \-os /var/tmp
.Ve
.PP
If you exported the guest as a \*(L"Folder of files\*(R", \fIor\fR if you unpacked the
\&\s-1OVA\s0 tarball yourself, then you can point virt\-v2v at the directory
containing the files:
.PP
.Vb 1
\& $ virt\-v2v \-i ova /path/to/files \-o local \-os /var/tmp
.Ve
.SH "ВВЕДЕННЯ З RHEL 5 XEN"
.IX Header "ВВЕДЕННЯ З RHEL 5 XEN"
Virt\-v2v is able to import Xen guests from \s-1RHEL 5\s0 Xen hosts.
.PP
Virt\-v2v uses libvirt for access to the remote Xen host, and therefore the
input mode should be \fI\-i libvirt\fR.  As this is the default, you don't need
to specify it on the command line.
.SS "\s-1XEN: SET UP\s0 SSH-AGENT \s-1ACCESS TO XEN HOST\s0"
.IX Subsection "XEN: SET UP SSH-AGENT ACCESS TO XEN HOST"
Currently you must enable passwordless \s-1SSH\s0 access to the remote Xen host
from the virt\-v2v conversion server.
.PP
You must also use ssh-agent, and add your ssh public key to
\&\fI/root/.ssh/authorized_keys\fR (on the Xen host).
.PP
After doing this, you should check that passwordless access works from the
virt\-v2v server to the Xen host.  For example:
.PP
.Vb 2
\& $ ssh root@xen.example.com
\& [ logs straight into the shell, no password is requested ]
.Ve
.PP
Note that password-interactive and Kerberos access are \fBnot\fR supported.
You \fBhave\fR to set up ssh access using ssh-agent and authorized_keys.
.SS "\s-1XEN: TEST LIBVIRT CONNECTION TO REMOTE XEN HOST\s0"
.IX Subsection "XEN: TEST LIBVIRT CONNECTION TO REMOTE XEN HOST"
Use the \fIvirsh\fR\|(1) command to list the guests on the remote Xen host:
.PP
.Vb 5
\& $ virsh \-c xen+ssh://root@xen.example.com list \-\-all
\&  Id    Name                           State
\& \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
\&  0     Domain\-0                       running
\&  \-     rhel49\-x86_64\-pv               shut off
.Ve
.PP
You should also try dumping the metadata from any guest on your server, like
this:
.PP
.Vb 5
\& $ virsh \-c xen+ssh://root@xen.example.com dumpxml rhel49\-x86_64\-pv
\& <domain type=\*(Aqxen\*(Aq>
\&   <name>rhel49\-x86_64\-pv</name>
\&   [...]
\& </domain>
.Ve
.PP
\&\fBIf the above commands do not work, then virt\-v2v is not going to work
either\fR.  Fix your libvirt configuration or the remote server before
continuing.
.SS "\s-1XEN: IMPORTING A GUEST\s0"
.IX Subsection "XEN: IMPORTING A GUEST"
To import a particular guest from a Xen server, do:
.PP
.Vb 3
\& $ virt\-v2v \-ic \*(Aqxen+ssh://root@xen.example.com\*(Aq \e
\&   rhel49\-x86_64\-pv \e
\&   \-o local \-os /var/tmp
.Ve
.PP
where \f(CW\*(C`rhel49\-x86_64\-pv\*(C'\fR is the name of the guest (which must be shut
down).
.PP
In this case the output flags are set to write the converted guest to a
temporary directory as this is just an example, but you can also write to
libvirt or any other supported target.
.SH "ВИВЕДЕННЯ ДО LIBVIRT"
.IX Header "ВИВЕДЕННЯ ДО LIBVIRT"
The \fI\-o libvirt\fR option lets you upload the converted guest to a
libvirt-managed host.  There are several limitations:
.IP "\(bu" 4
You can only use a local libvirt connection [see below for how to workaround
this].
.IP "\(bu" 4
The \fI\-os pool\fR option must specify a directory pool, not anything more
exotic such as iSCSI [but see below].
.IP "\(bu" 4
You can only upload to a \s-1KVM\s0 hypervisor.
.PP
\&\fBTo output to a remote libvirt instance and/or a non-directory storage
pool\fR you have to use the following workaround:
.IP "1." 4
Use virt\-v2v in \fI\-o local\fR mode to convert the guest disks and metadata
into a local temporary directory:
.Sp
.Vb 1
\& virt\-v2v [...] \-o local \-os /var/tmp
.Ve
.Sp
This creates two (or more) files in \fI/var/tmp\fR called:
.Sp
.Vb 2
\& /var/tmp/NAME.xml     # the libvirt XML (metadata)
\& /var/tmp/NAME\-sda     # the guest\*(Aqs first disk
.Ve
.Sp
(for \f(CW\*(C`NAME\*(C'\fR substitute the guest's name).
.IP "2." 4
Upload the converted disk(s) into the storage pool called \f(CW\*(C`POOL\*(C'\fR:
.Sp
.Vb 3
\& size=$(stat \-c%s /var/tmp/NAME\-sda)
\& virsh vol\-create\-as POOL NAME\-sda $size \-\-format raw
\& virsh vol\-upload \-\-pool POOL NAME\-sda /var/tmp/NAME\-sda
.Ve
.IP "3." 4
Edit \fI/var/tmp/NAME.xml\fR to change \fI/var/tmp/NAME\-sda\fR to the pool name.
In other words, locate the following bit of \s-1XML:\s0
.Sp
.Vb 5
\& <disk type=\*(Aqfile\*(Aq device=\*(Aqdisk\*(Aq>
\&   <driver name=\*(Aqqemu\*(Aq type=\*(Aqraw\*(Aq cache=\*(Aqnone\*(Aq />
\&   <source file=\*(Aq/var/tmp/NAME\-sda\*(Aq />
\&   <target dev=\*(Aqhda\*(Aq bus=\*(Aqide\*(Aq />
\& </disk>
.Ve
.Sp
and change two things: The \f(CW\*(C`type=\*(Aqfile\*(Aq\*(C'\fR attribute must be changed to
\&\f(CW\*(C`type=\*(Aqvolume\*(Aq\*(C'\fR, and the \f(CW\*(C`<source>\*(C'\fR element must be changed to
include \f(CW\*(C`pool\*(C'\fR and \f(CW\*(C`volume\*(C'\fR attributes:
.Sp
.Vb 5
\& <disk type=\*(Aqvolume\*(Aq device=\*(Aqdisk\*(Aq>
\&   ...
\&   <source pool=\*(AqPOOL\*(Aq volume=\*(AqNAME\-sda\*(Aq />
\&   ...
\& </disk>
.Ve
.IP "4." 4
Define the final guest in libvirt:
.Sp
.Vb 1
\& virsh define /var/tmp/NAME.xml
.Ve
.SH "OUTPUT TO RHEV"
.IX Header "OUTPUT TO RHEV"
This section only applies to the \fI\-o rhev\fR output mode.  If you use
virt\-v2v from the RHEV-M user interface, then behind the scenes the import
is managed by \s-1VDSM\s0 using the \fI\-o vdsm\fR output mode (which end users should
not try to use directly).
.PP
You have to specify \fI\-o rhev\fR and an \fI\-os\fR option that points to the
RHEV-M Export Storage Domain.  You can either specify the \s-1NFS\s0 server and
mountpoint, eg. \f(CW\*(C`\-os rhev\-storage:/rhev/export\*(C'\fR, or you can mount that
first and point to the directory where it is mounted, eg. \f(CW\*(C`\-os /tmp/mnt\*(C'\fR.  Be careful not to point to the Data Storage Domain by accident
as that will not work.
.PP
On successful completion virt\-v2v will have written the new guest to the
Export Storage Domain, but it will not yet be ready to run.  It must be
imported into \s-1RHEV\s0 using the \s-1UI\s0 before it can be used.
.PP
In \s-1RHEV\s0 ≥ 2.2 this is done from the Storage tab.  Select the export
domain the guest was written to.  A pane will appear underneath the storage
domain list displaying several tabs, one of which is \*(L"\s-1VM\s0 Import\*(R".  The
converted guest will be listed here.  Select the appropriate guest an click
\&\*(L"Import\*(R".  See the \s-1RHEV\s0 documentation for additional details.
.PP
If you export several guests, then you can import them all at the same time
through the \s-1UI.\s0
.SH "RESOURCE REQUIREMENTS"
.IX Header "RESOURCE REQUIREMENTS"
.SS "Мережа"
.IX Subsection "Мережа"
The most important resource for virt\-v2v appears to be network bandwidth.
Virt\-v2v should be able to copy guest data at gigabit ethernet speeds or
greater.
.PP
Ensure that the network connections between servers (conversion server, \s-1NFS\s0
server, vCenter, Xen) are as fast and as low latency as possible.
.SS "Місце на диску"
.IX Subsection "Місце на диску"
Virt\-v2v places potentially large temporary files in \f(CW$TMPDIR\fR (which is
\&\fI/var/tmp\fR if you don't set it).  Using tmpfs is a bad idea.
.PP
For each guest disk, an overlay is stored temporarily.  This stores the
changes made during conversion, and is used as a cache.  The overlays are
not particularly large \- tens or low hundreds of megabytes per disk is
typical.  In addition to the overlay(s), input and output methods may use
disk space, as outlined in the table below.
.IP "\fI\-i ova\fR" 4
.IX Item "-i ova"
This temporarily places a full copy of the uncompressed source disks in
\&\f(CW$TMPDIR\fR.
.IP "\fI\-o glance\fR" 4
.IX Item "-o glance"
This temporarily places a full copy of the output disks in \f(CW$TMPDIR\fR.
.IP "\fI\-o local\fR" 4
.IX Item "-o local"
.PD 0
.IP "\fI\-o qemu\fR" 4
.IX Item "-o qemu"
.PD
You must ensure there is sufficient space in the output directory for the
converted guest.
.IP "\fI\-o null\fR" 4
.IX Item "-o null"
This temporarily places a full copy of the output disks in \f(CW$TMPDIR\fR.
.SS "Ресурси vCenter VMware"
.IX Subsection "Ресурси vCenter VMware"
Copying from VMware vCenter is currently quite slow, but we believe this to
be an issue with VMware.  Ensuring the VMware ESXi hypervisor and vCenter
are running on fast hardware with plenty of memory should alleviate this.
.SS "Compute power and \s-1RAM\s0"
.IX Subsection "Compute power and RAM"
Virt\-v2v is not especially compute or \s-1RAM\s0 intensive.  If you are running
many parallel conversions, then you may consider allocating one \s-1CPU\s0 core and
between 512 \s-1MB\s0 and 1 \s-1GB\s0 of \s-1RAM\s0 per running instance.
.PP
Virt\-v2v можна запускати у віртуальній машині.
.SH "ЗАВДАННЯ ПІСЛЯ ПЕРЕТВОРЕННЯ"
.IX Header "ЗАВДАННЯ ПІСЛЯ ПЕРЕТВОРЕННЯ"
.SS "Налаштовування гостьової мережі"
.IX Subsection "Налаштовування гостьової мережі"
Virt\-v2v cannot currently reconfigure a guest's network configuration.  If
the converted guest is not connected to the same subnet as the source, its
network configuration may have to be updated.  See also
\&\fIvirt\-customize\fR\|(1).
.SS "Перетворення гостьової системи Windows"
.IX Subsection "Перетворення гостьової системи Windows"
When converting a Windows guests, the conversion process is split into two
stages:
.IP "1." 4
Offline conversion.
.IP "2." 4
Перше завантаження.
.PP
The guest will be bootable after the offline conversion stage, but will not
yet have all necessary drivers installed to work correctly.  These will be
installed automatically the first time the guest boots.
.PP
\&\fBN.B.\fR Take care not to interrupt the automatic driver installation process
when logging in to the guest for the first time, as this may prevent the
guest from subsequently booting correctly.
.SH "FREE SPACE FOR CONVERSION"
.IX Header "FREE SPACE FOR CONVERSION"
Virt\-v2v checks there is sufficient free space in the guest filesystem to
perform the conversion.  Currently it checks:
.ie n .IP "Linux root filesystem or Windows ""C:"" drive" 4
.el .IP "Linux root filesystem or Windows \f(CWC:\fR drive" 4
.IX Item "Linux root filesystem or Windows C: drive"
Мінімальний вільний простір: 20 МБ
.IP "Linux \fI/boot\fR" 4
.IX Item "Linux /boot"
Мінімальний вільний простір: 50 МБ
.Sp
This is because we need to build a new initramfs for some Enterprise Linux
conversions.
.IP "Будь\-яка інша придатна до монтування файлова система" 4
.IX Item "Будь-яка інша придатна до монтування файлова система"
Мінімальний вільний простір: 10 МБ
.SH "RUNNING VIRT\-V2V AS ROOT OR NON-ROOT"
.IX Header "RUNNING VIRT-V2V AS ROOT OR NON-ROOT"
Nothing in virt\-v2v inherently needs root access, and it will run just fine
as a non-root user.  However, certain external features may require either
root or a special user:
.IP "Mounting the Export Storage Domain" 4
.IX Item "Mounting the Export Storage Domain"
When using \fI\-o rhev \-os server:/esd\fR virt\-v2v has to have sufficient
privileges to \s-1NFS\s0 mount the Export Storage Domain from \f(CW\*(C`server\*(C'\fR.
.Sp
You can avoid needing root here by mounting it yourself before running
virt\-v2v, and passing \fI\-os /mountpoint\fR instead, but first of all read the
next section ...
.IP "Writing to the Export Storage Domain as 36:36" 4
.IX Item "Writing to the Export Storage Domain as 36:36"
RHEV-M cannot read files and directories from the Export Storage Domain
unless they have \s-1UID:GID 36:36. \s0 You will see \s-1VM\s0 import problems if the
\&\s-1UID:GID\s0 is not correct.
.Sp
When you run virt\-v2v \fI\-o rhev\fR as root, virt\-v2v attempts to create files
and directories with the correct ownership.  If you run virt\-v2v as
non-root, it will probably still work, but you will need to manually change
ownership after virt\-v2v has finished.
.IP "Запис до libvirt" 4
.IX Item "Запис до libvirt"
When using \fI\-o libvirt\fR, you may need to run virt\-v2v as root so that it
can write to the libvirt system instance (ie. \f(CW\*(C`qemu:///system\*(C'\fR)  and to the
default location for disk images (usually \fI/var/lib/libvirt/images\fR).
.Sp
You can avoid this by setting up libvirt connection authentication, see
http://libvirt.org/auth.html.  Alternatively, use \fI\-oc qemu:///session\fR,
which will write to your per-user libvirt instance.
.IP "Запис до Glance" 4
.IX Item "Запис до Glance"
This does \fInot\fR need root (in fact it probably won't work), but may require
either a special user and/or for you to source a script that sets
authentication environment variables.  Consult the Glance documentation.
.SH "DEBUGGING RHEV-M IMPORT FAILURES"
.IX Header "DEBUGGING RHEV-M IMPORT FAILURES"
When you export to the RHEV-M Export Storage Domain, and then import that
guest through the RHEV-M \s-1UI,\s0 you may encounter an import failure.
Diagnosing these failures is infuriatingly difficult as the \s-1UI\s0 generally
hides the true reason for the failure.
.PP
There are two log files of interest.  The first is stored on the RHEV-M
server itself, and is called \fI/var/log/ovirt\-engine/engine.log\fR
.PP
The second file, which is the most useful, is found on the \s-1SPM\s0 host (\s-1SPM\s0
stands for \*(L"Storage Pool Manager\*(R").  This is a \s-1RHEV\s0 node that is elected to
do all metadata modifications in the data center, such as image or snapshot
creation.  You can find out which host is the current \s-1SPM\s0 from the \*(L"Hosts\*(R"
tab \*(L"Spm Status\*(R" column.  Once you have located the \s-1SPM,\s0 log into it and
grab the file \fI/var/log/vdsm/vdsm.log\fR which will contain detailed error
messages from low-level commands.
.SH "МІНІМАЛЬНИЙ XML ДЛЯ ПАРАМЕТРА \-i libvirtxml"
.IX Header "МІНІМАЛЬНИЙ XML ДЛЯ ПАРАМЕТРА -i libvirtxml"
When using the \fI\-i libvirtxml\fR option, you have to supply some libvirt
\&\s-1XML. \s0 Writing this from scratch is hard, so the template below is helpful.
.PP
\&\fBNote this should only be used for testing and/or where you know what
you're doing!\fR If you have libvirt metadata for the guest, always use that
instead.
.PP
.Vb 10
\& <domain type=\*(Aqkvm\*(Aq>
\&   <name>NAME</name>
\&   <memory>1048576</memory>
\&   <vcpu>2</vcpu>
\&   <os>
\&     <type>hvm</type>
\&     <boot dev=\*(Aqhd\*(Aq/>
\&   </os>
\&   <features>
\&     <acpi/>
\&     <apic/>
\&     <pae/>
\&   </features>
\&   <devices>
\&     <disk type=\*(Aqfile\*(Aq device=\*(Aqdisk\*(Aq>
\&       <driver name=\*(Aqqemu\*(Aq type=\*(Aqraw\*(Aq/>
\&       <source file=\*(Aq/path/to/disk/image\*(Aq/>
\&       <target dev=\*(Aqhda\*(Aq bus=\*(Aqide\*(Aq/>
\&     </disk>
\&     <interface type=\*(Aqnetwork\*(Aq>
\&       <mac address=\*(Aq52:54:00:01:02:03\*(Aq/>
\&       <source network=\*(Aqdefault\*(Aq/>
\&       <model type=\*(Aqrtl8139\*(Aq/>
\&     </interface>
\&   </devices>
\& </domain>
.Ve
.SH "MACHINE READABLE OUTPUT"
.IX Header "MACHINE READABLE OUTPUT"
The \fI\-\-machine\-readable\fR option can be used to make the output more machine
friendly, which is useful when calling virt\-v2v from other programs, GUIs
etc.
.PP
Існує два способи використання цього параметра.
.PP
Firstly use the option on its own to query the capabilities of the virt\-v2v
binary.  Typical output looks like this:
.PP
.Vb 9
\& $ virt\-v2v \-\-machine\-readable
\& virt\-v2v
\& libguestfs\-rewrite
\& input:disk
\& [...]
\& output:local
\& [...]
\& convert:enterprise\-linux
\& convert:windows
.Ve
.PP
A list of features is printed, one per line, and the program exits with
status 0.
.PP
The \f(CW\*(C`input:\*(C'\fR and \f(CW\*(C`output:\*(C'\fR features refer to \fI\-i\fR and \fI\-o\fR (input and
output mode) options supported by this binary.  The \f(CW\*(C`convert:\*(C'\fR features
refer to guest types that this binary knows how to convert.
.PP
Secondly use the option in conjunction with other options to make the
regular program output more machine friendly.
.PP
У поточній версії це означає таке:
.IP "1." 4
Progress bar messages can be parsed from stdout by looking for this regular
expression:
.Sp
.Vb 1
\& ^[0\-9]+/[0\-9]+$
.Ve
.IP "2." 4
The calling program should treat messages sent to stdout (except for
progress bar messages) as status messages.  They can be logged and/or
displayed to the user.
.IP "3." 4
The calling program should treat messages sent to stderr as error messages.
In addition, virt\-v2v exits with a non-zero status code if there was a fatal
error.
.PP
Virt\-v2v ≤ 0.9.1 did not support the \fI\-\-machine\-readable\fR option at
all.  The option was added when virt\-v2v was rewritten in 2014.
.SH "ФАЙЛИ"
.IX Header "ФАЙЛИ"
.IP "\fI/usr/share/virtio\-win\fR" 4
.IX Item "/usr/share/virtio-win"
(Необов’язково)
.Sp
If this directory is present, then virtio drivers for Windows guests will be
found from this directory and installed in the guest during conversion.
.SH "ЗМІННІ СЕРЕДОВИЩА"
.IX Header "ЗМІННІ СЕРЕДОВИЩА"
.ie n .IP """TMPDIR""" 4
.el .IP "\f(CWTMPDIR\fR" 4
.IX Item "TMPDIR"
Location of the temporary directory used for the potentially large temporary
overlay file.
.Sp
See the \*(L"Disk space\*(R" section above.
.ie n .IP """VIRT_TOOLS_DATA_DIR""" 4
.el .IP "\f(CWVIRT_TOOLS_DATA_DIR\fR" 4
.IX Item "VIRT_TOOLS_DATA_DIR"
This can point to the directory containing data files used for Windows
conversion.
.Sp
Normally you do not need to set this.  If not set, a compiled-in default
will be used (something like \fI/usr/share/virt\-tools\fR).
.Sp
Цей каталог може містити такі файли:
.RS 4
.IP "\fIrhsrvany.exe\fR" 4
.IX Item "rhsrvany.exe"
(Required when doing conversions of Windows guests)
.Sp
This is the RHSrvAny Windows binary, used to install a \*(L"firstboot\*(R" script in
the guest during conversion of Windows guests.
.Sp
Див. також \f(CW\*(C`https://github.com/rwmjones/rhsrvany\*(C'\fR
.IP "\fIrhev\-apt.exe\fR" 4
.IX Item "rhev-apt.exe"
(Необов’язково)
.Sp
The \s-1RHEV\s0 Application Provisioning Tool (\s-1RHEV APT\s0).  If this file is present,
then \s-1RHEV APT\s0 will be installed in the Windows guest during conversion.
This tool is a guest agent which ensures that the virtio drivers remain up
to date when the guest is running on Red Hat Enterprise Virtualization
(\s-1RHEV\s0).
.Sp
This file comes from Red Hat Enterprise Virtualization (\s-1RHEV\s0), and is not
distributed with virt\-v2v.
.RE
.RS 4
.RE
.ie n .IP """VIRTIO_WIN""" 4
.el .IP "\f(CWVIRTIO_WIN\fR" 4
.IX Item "VIRTIO_WIN"
This is where VirtIO drivers for Windows are searched for
(\fI/usr/share/virtio\-win\fR if unset).  It can be a directory \fIor\fR point to
\&\fIvirtio\-win.iso\fR (\s-1CD ROM\s0 image containing drivers).
.Sp
See \*(L"\s-1ENABLING VIRTIO\*(R"\s0.
.PP
Опис інших змінних середовища наведено у розділі \*(L"\s-1ENVIRONMENT
VARIABLES\*(R"\s0 in \fIguestfs\fR\|(3).
.SH "ТАКОЖ ПЕРЕГЛЯНЬТЕ"
.IX Header "ТАКОЖ ПЕРЕГЛЯНЬТЕ"
\&\fIvirt\-p2v\fR\|(1), \fIvirt\-customize\fR\|(1), \fIvirt\-df\fR\|(1), \fIvirt\-filesystems\fR\|(1),
\&\fIvirt\-sparsify\fR\|(1), \fIvirt\-sysprep\fR\|(1), \fIguestfs\fR\|(3), \fIguestfish\fR\|(1),
\&\fIqemu\-img\fR\|(1), \fIfstrim\fR\|(8), \fIvirt\-v2v\-test\-harness\fR\|(1),
http://libguestfs.org/.
.SH "АВТОРИ"
.IX Header "АВТОРИ"
Richard W.M. Jones http://people.redhat.com/~rjones/
.PP
Matthew Booth
.PP
Mike Latimer
.PP
Pino Toscano
.PP
Shahar Havivi
.PP
Tingting Zheng
.SH "АВТОРСЬКІ ПРАВА"
.IX Header "АВТОРСЬКІ ПРАВА"
© Red Hat Inc., 2009–2015
.SH "LICENSE"
.IX Header "LICENSE"
.SH "BUGS"
.IX Header "BUGS"
To get a list of bugs against libguestfs, use this link:
https://bugzilla.redhat.com/buglist.cgi?component=libguestfs&product=Virtualization+Tools
.PP
To report a new bug against libguestfs, use this link:
https://bugzilla.redhat.com/enter_bug.cgi?component=libguestfs&product=Virtualization+Tools
.PP
When reporting a bug, please supply:
.IP "\(bu" 4
The version of libguestfs.
.IP "\(bu" 4
Where you got libguestfs (eg. which Linux distro, compiled from source, etc)
.IP "\(bu" 4
Describe the bug accurately and give a way to reproduce it.
.IP "\(bu" 4
Run \fIlibguestfs\-test\-tool\fR\|(1) and paste the \fBcomplete, unedited\fR
output into the bug report.
